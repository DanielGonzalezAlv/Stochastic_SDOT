\documentclass[
     12pt,         % font size
     a4paper,      % paper format
     BCOR=10mm,     % binding correction
     DIV=14,        % stripe size for margin calculation
%     liststotoc,   % table listing in toc
%     bibtotoc,     % bibliography in toc
%     idxtotoc,     % index in toc
%     parskip       % paragraph skip instad of paragraph indent
     ]{scrreprt}

\input{../preamble.tex}

% Avoid Problems by including other files
\usepackage{standalone}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
    
    % Nomenclature
    \input{/home/daniel/Studies/MA/thesis/main/Nomenclature/nomenclature} 
    %\newpage


    % Genearal equations / Motivation
    \input{/home/daniel/Studies/MA/thesis/main/General_Equations/general_equations}
    %
    
    For a given finite set $S \subset \R^d$, we want to study the following problem: \\ 
    Given two probability spaces $(\R^d$, $\B^d$, $\mu)$ and $(S, \Pot(S), \nu)$, we want to minimize 
    %
    \begin{align} \label{eq::main}
        \integral {\R^d} {\|x-T(x) \|^2} {\mu}  
    \end{align}
    %
    over all measurable maps $T: \R^d \to S$ satisfying $T_{\#}\mu = \nu$. \\%[8pt]
    %
    Recall that as we are working with a finite probability space $(S, \Pot(S), \nu)$, we can write $\nu$ as a finite sum of Dirac measures
    \begin{align} \label{eq::dirac} 
        \nu = \sum_{s\in S} {\nu_s \delta_{s}} \quad \text{where   } \nu_s \in \R_{\ge 0} \text{   and such that} \quad \sum_{s \in S}{\nu_s} = 1. 
    \end{align}
    %
    Thus, the problem we are considering translates to find a measurable map $T$ that minimizes the functional in (\ref{eq::main}) and such that 
    $\mu(T^{-1}(s)) \coloneqq \mu(T^{-1}(\{s\})) = \nu_s $  for all $s\in S$. For any $s\in S$, we call $\mu(T^{-1}(s))$  the \textit{capacity} of $s$.
    %
    \TODO{** Prove existence and uniqueness of a solution}
    %
    \indent Following the exposition in [AHA], we will now show that finding a minimizer of the functional (\ref{eq::main}) is equivalent to finding the maximum of a concave function and thus, can be solved with
    standard optimization methods. The formulation of this optimization problem is done in two steps.
    First, we find a minimizer over all measurable functions with equal capacities. The optimal solution $T_W$ is inspired geometrically and constructed using 
    a predefined \textit{weight vector}  $W$.\\
    %This weights are defined for every point in $S$ and have a geometrical interpretation, which we will describe below. 
    The next step will consist in adapting this weight vector, such that the condition ${T_{W}}_{\#}\mu = \nu$ is fullfilled.\\ 
    The motivation behind this approach is inspired geometrically by studying a generalization of \textit{Voroni diagrams}. 
    For clarity, we recall the definition of these diagrams and review the necessary concepts needed for this approach. 
    %
    \begin{defi}[Voronoi Diagrams]
        Let $S\subset \R^d$ be a finite set.  We define for every point $s\in S$
        \[\reg(s) \coloneqq \{x \in \R^d : \|x-s\| \le \|x-\tilde s \| \quad  \forAll \tilde s \in S\setminus \{s\} \}. \]
        We call this the \textit{region} or \textit{cell} of the point $s$.
        The partition of $\R^d$ created by the union of the regions of all points is called the Voronoi diagram of $S$.
    \end{defi}
    %
    %
    %
    \begin{rem}
        Note that the partition of $\R^d$ generated by the Voronoi diagram of $S\subset \R^d$, is given by convex regions.  Such a partition induces naturally a map $T: \R^d \to S$, which assign each point in $\R^d$ the corresponding 
        point in $S$ of the cell where it is located, i.e
         \begin{align} \label{eq::reg}
             T(x) = s \quad \Leftrightarrow \quad x\in \reg(s).  
         \end{align}
         By definition, some points in $\R^d$ may belong to more than one region.  By convention, $T$ assigns those points an arbitrary one in $S$ of a region
         where it is located. We call $T$, the by the \textit{Voronoi diagram} induced assigment. 
    \end{rem}
    %
    %
    %
    A generalization of the presented concepts arise when using another distance function for the definition of the regions. 
    %This allow us to vary the induced assignment.
    One application of this, amounts to using the \textit{power function} with weights $W$ which we now define.

    \begin{defi}[Power function]      
        Let $S\subset \R^d$ be a finite set and $W: S \to \R $ a function on $S$. The power function with weights $W$ is defined as
        \[ \pow_W(x,s) \coloneqq \|x-s\|^2 - W(s). \]
        We call $W$ the \textit{weight function} on S.
    \end{defi}
    %
    %
    %
    \begin{rem}
        For simplicity of notation we will write sometimes the weight function \\ $W:S\to\R^d$ as a vector in $\R^{|S|}$. We will then call $W$ the \textit{weight vector} or simply the \textit{weights} of S.
    \end{rem}
    %
    %
    As in the case of Voronoi diagrams, we can define regions on $\R^d$ by using the power function with weights $W$.
    For a point $s\in S$ we call 
    \[\reg_W(s) \coloneqq \{x \in \R^d : \pow_W(x,s) \le \pow_W(x,\tilde s) \quad  \forall \tilde s \in S\setminus \{s\} \} \]
    the \textit{power region} (or power cell) of $s$ with weights $W$. Power regions also create a partition of $\R^d$ which is called the \textit{power diagram} of $S$ with weights $W$. \\
    The geometric intution behind the definition of power diagrams becomes clear by looking at spheres around $s\in S$ with positive radius %$\sqrt {W(s)}$
    %
    \[\Sph_{\sqrt W}^{d-1}(s) \coloneqq \{x \in \R^d : \| x-s \| = \sqrt{W(s)} \} \]
    %
    whenever $W : S \to \R_{>0} $. The power function $\pow(\cdot, s)$ for a fixed $s\in S$, returns a negative (resp. positive) value, whenever $x\in \R^d$ is inside 
    (resp. outside) the sphere $\Sph_{\sqrt W}^{d-1}(s)$ and zero when $s$ lies on the sphere. Thus, increasing (resp. decreasing) the values of the weights $W(s)$ on each point $s$ would expand (resp. schrink)
    the power cells. 
    %
    %
    \begin{rem}
        Unlike Voronoi diagrams, the power cells of a point $s\in S$ may not contain the point $s$ or even may be empty. Nevertheless, the power diagram still partitions $\R^d$ in 
        convex polyhedron.
    \end{rem}
    %
    %
    By replacing $\reg$ with $\reg_W$ in (\ref{eq::reg}) we obtain a map $T_W: \R^d \to S$ depending on the weight vector $W$.  Similarly as with Voronoi diagrams, we assign those points sharing
    different cells, an arbitrarly point $s\in S$ of those shared regions. We call this map, the \textit{power assignment} of $S$ with weights $W$. \\
    %
    Power assignments have a natural optimization property, since by definition it holds 
    %an optimization property, which we recall in the following Lemma.
    \begin{align}
        ( T_W(x) = s \  \Leftrightarrow \ x\in \reg_W(s) ) \quad \Leftrightarrow \quad T_W(x) = \argmin_{s\in S} \|x-s\|^2 - W(s) \label{eq::minimalityPower}
    \end{align}
    %
    for all points $x \in \R^d$ which don't share different regions. In fact, power functions even minimize the functional (\ref{eq::main}) for a fixed predefined weight vector $W$.
    We will prove this in Lemma \ref{lemma::1step}. As a consequence, the natural question which remains to be clarified is how to fix a choice of the weight vector $W$, such that it fullfills the condition ${T_{W}}_{\#}\mu = \nu$.\\[8pt]
    %
    \indent We recall the change of variables theorem from measure theory. 
    %
    \begin{thm*}[change of variables]
         Let $(X, \A, \mu)$ be a measure space, $(Y, \U)$ a measurable space and $T: X \to Y $ a measurable function.\\
         For a measurable function $f : Y \to \R^d $ the following are equivalent
         \begin{itemize}
             \item[(i)] $f\in \LL(T_\#\mu)$ 
             \item[(ii)]  $f \circ T \in \LL(\mu) $ 
        \end{itemize}         
        In case any of these statements is true we have also
         %
         \begin{align} \label{eq::transformel}
             \integral {T^{-1}(B)} {f} {T_{\#}\mu} = \integral {B} {f \circ T} {\mu} \quad \text{for all } B\in \U.
         \end{align}
         %
    \end{thm*}
    % 
    \begin{proof}[Proof]
    Measure theory, e.g p.$191$ [J.E]
    \end{proof}
    % Seite 191 Measure theory
    
%    %
%    $x \mapsto \argmin_{s\in S} \|x-s\|. $  Note that some points   
%
%    all measurable functions
%    $T: \R^d \to S$ satisfying $\mu(T^{-1}(s)) = w_s$, where $w_s \in \R$ are fixed values defined for every point $s$ in $S$. 
%    \TODO{ ....}
%
%
%    \TODO{ formulate differnt this lemma}
    %
    \begin{rem*} \label{rem::measurabilty}
        Note that as the power region of a point $s\in S\subset \R^d$ is either an empty set or a convex polyhedra, it is measurable with respect to the Lebesgue measure $\lambda^d$ on $\R^d$.
        Denoting by $\interior(B)$ the interior of a set $B \in \B$ with respect to the standard topology, we know that  
        %
        \[\lambda^d(\reg(s)) = \lambda^d({\interior(\reg(s)})).\]
        %
        For a probability space $(\R^d,\B^d,\mu)$ such that $\mu \ll \lambda^d$, we have then
        \[\mu(\reg(s)) = \mu(\interior({\reg(s)})) \quad \quad \text{and} \quad \quad \sum_{s\in S} \mu(\reg(s))=1.\]

        
        %exists by the Radon-Nikodym a non-negative function $f\in \LL(\mu)$, s.t 
        %\[\mu (B) = \integral {B} {f} {\mu} \quad \quad \forall B\in \B^d. \]
        %%
        %Thus, it holds 
    \end{rem*}
    %
    %
    \begin{lem} \label{lemma::1step}
        Let $(\R^d,\B,\mu)$ be a probability space, such that $\mu \ll \lambda^d$.  Let $S$ be a finite subset of $\R^d$ with weights $W$ and $\zeta: S \to \R_{\ge 0}$ be a function on S.
        Then, the power assignment $T_W $ minimizes 
        %\[\integral {R^D} {\rho(x) \|x-T(x) \|} {\lambda^d} \]
        \[\integral {\R^d} {\|x-T(x) \|^2} {\mu} \]
        over all measurable maps $T : \R^d \to S $ with capacities $\mu(T^{-1} (s)) = \zeta(s)  $ for all $s\in S$.
    \end{lem}
    %
    \begin{proof}[Proof]
        Using the minimality condition (\ref{eq::minimalityPower}) of power assignments, we see that for a fixed $x\in \R^d$ holds
        \[\pow_W(x,T_W(x)) \le \pow_W(x,s) \quad \forAll s\in S. \] 
        Consequently, $T_W$ minimizes 
        \[ \integral {\R^d} {\pow_W(x,T(x))} {\mu} =  \integral{\R^d} {\| x - T(x) \|^2} {\mu} - \integral{\R^d} {W(T(x))} {\mu} \]
%        \begin{align*} 
%            \integral {\R^D} {\pow_W(x,T(x))} {\mu} &=  \integral{\R^d} {\| x - T(x) \|^2} {\mu} - \integral{\R^d} {\omega(T(x))} {\mu}\\
%                                                    &\overset {\text{explain}}=  \integral{\R^d} {\rho(x)\| x - T(x) \|^2} {\lambda^d} - \integral{\R^d} {\omega(T(x))} {\mu}
%        \end{align*}
        over all measurable maps $T:\R^d \to S$. Using the fact that $\R^d = \bigcup_{s\in S} \reg(s)$ together with remark \ref{rem::measurabilty}, we obtain
        %and that the boundaries of the power cells have zero $\lambda^d$-measure and thus also zero $\mu$-measure, it holds
        \begin{align*} 
            %\integral{\R^d} {\omega(T(x))} {\mu} &\overset {(\ref{eq::transformel})}=  \sum_{s\in S} \integral{s} {\omega(T(x))} {\mu^T} 
            %                                     = \sum_{s\in S} \integral{s} {\omega(T(x))} {\mu^T} \\
            %                                     &= \sum_{s\in S} \mu(T^{-1}(s))\omega(s) \text{ \ which is constant}
            \integral{\R^d} {W(T(x))} {\mu} &= \sum_{s\in S} \integral{\reg(s)} {W(T(x))} {\mu} \\ 
                                                 &\overset {(\ref{eq::transformel})}= \sum_{s\in S} \integral{s} {W} {T_{\#}\mu} \\
                                                 &= \sum_{s\in S} \mu(T^{-1}(s))W(s) \\
                                                 &= \sum_{s\in S} \zeta(s)W(s) 
        \end{align*}
        which is constant for a fixed $\zeta$ and $W$.
    \end{proof}

    The natural question to handle next, is how to choose $W$, such that the condition ${T_{W}}_{\#}\mu = \nu$ holds. As we will show below, this question can be equivalently formulated
    as finding the maximum of a concave function. In order to achive this, we first recall the original setting of our original problem and introduce some definitions. \\[8pt]
    %
    \indent Let $(\R^d, \B, \mu)$ and $(S, \Pot(S), \nu)$ be two probability spaces such that $\mu \ll \lambda^d$. As in equation (\ref{eq::dirac}), we write the measure $\nu$ as 
    a finite sum of Dirac measures $\nu = \sum_{s\in S} {\nu_s \delta_{s}} $. \\
    For $\FF \coloneqq \{ f: \R^d \to S : f \text{ is measurable}\}$, define
    %
    \[L: \FF \times \R^{|S|} \to \R, \quad (T,W) \mapsto \integral {\R^d} {\pow_W(x,T(x))} {\mu}. \]
    %
    This map has important properties, which we will show below and use for reformulation of our original problem into a concave optimization problem. For a map $T\in \FF$, let 
    %
    \begin{align} \label{eq::defzeta}
         \zeta_T : S \to \R, \quad s\mapsto \mu(T^{-1}(s))
    \end{align}
    be the vector of capacities induced by $T$ and 
    %
    \[Q: \FF \to \R, \quad T \mapsto \integral {\R^d} {\|x-T(x)\|^2} {\mu} \]
    %
    be the functional that we want to study. As shown in Lemma \ref{lemma::1step}, it holds
    %
    \begin{align} \label{eq::defL}
        L(T,W) = Q(T) - \langle \zeta_T, W \rangle.
    \end{align}
    %
    And hence, $L_T \coloneqq L(T, \cdot)$ defines a linear function on $\R^{|S|}$ for any fixed $T \in \FF$. \\[8pt]
    %
    %
    %
    \indent Recall that for a given $W \in \R^{|S|}$ and $x\in \R^d$, the minimality condition for power assignments (\ref{eq::minimalityPower}) states
    \[\pow_W(x,T_W(x)) \le \pow_W(x,s) \quad \forAll s \in S.\]
    Consequently, for a fixed $W \in \R^d$ we must have 
    %
    %
    \begin{align} \label{eq:propTW}
        T_W = \argmin_{T\in \FF} L(T,W).
    \end{align}
    %
    %
    We claim that \[ f : \R^{|S|} \to \R^d, \quad W \mapsto L(T_W, W) = L_{T_W}(W) \]
    is smooth and concave.
    %\TODO{Rewrite properties for the proof}
    \begin{proof}[Proof]
        We prove first the differentiability of $f$ at every point of $\R^{|S|}$. Recall that for this, we have to show for every point $W\in \R^{|S|}$ the existence of a linear function ${Df_{W} : \R^{|S|}
        \to \R}$  
        which satisfies
        \[\lim_{h \to 0}\frac{|f(W+h) - f(W) - Df_{W}(h) |}{\|h\|} = 0. \]
        We achive this by using $Df_{W} : h \mapsto -\langle \zeta_{T_W}, h \rangle $, for capacity vectors $\zeta_{T_W}$ as defined in (\ref{eq::defzeta}). Then, this function is linear and satisfies
        \begin{align*}
            \frac{|f(W+h) - f(W) - L(h) |}{\|h\|} &= \frac{|L_{T_{W+h}}(W+h) - L_{T_{W}}(W) + \langle \zeta_{T_W}, h \rangle |}{\|h\|} \\ 
                                                &\overset {\text{(\ref{eq:propTW})}}\le \frac{|L_{T_{W}}(W+h) - L_{T_{W}}(W) + \langle \zeta_{T_W}, h \rangle |}{\|h\|} \\
                                                &\overset {\text{lin}}= \frac{|L_{T_{W}}(W) + L_{T_{W}}(h) - L_{T_{W}}(W) + \langle \zeta_{T_W}, h \rangle |}{\|h\|} \\
                                                &\overset {\text{(\ref{eq::defL})}}= \frac{|Q(T_W)|}{\|h\|} \quad \overset{\|h\|\to 0}\longrightarrow \quad 0.
        \end{align*}
        We show now the convexity of $f$. Let $\alpha \in [0,1]$ and $W_1, W_2 \in \R^{|S|}$, then
        \begin{align*}
            f(\alpha W_1 + (1- \alpha)W_2) &= L_{T_{\alpha W_1 + (1- \alpha)W_2}}(\alpha W_1 + (1- \alpha)W_2) \\
                                        &\overset {\text{lin}}= L_{T_{\alpha W_1 + (1- \alpha)W_2}}(\alpha W_1) + L_{T_{\alpha W_1 + (1- \alpha)W_2}}((1-\alpha) W_2)\\
                                        &\overset {\text{(\ref{eq:propTW})}}\ge L_{T_{\alpha W_1}}(\alpha W_1) + L_{T_{(1-\alpha) W_2}}((1- \alpha) W_2) \\
                                        &\overset {\text{lin}}= \alpha L_{T_{\alpha W_1}}(W_1) + (1-\alpha) L_{T_{(1-\alpha) W_2}}(W_2) \\ 
                                        &\overset {\text{(\ref{eq:propTW})}}\ge \alpha L_{T_{W_1}}(W_1) + (1-\alpha) L_{T_{W_2}}(W_2) = \alpha f(W_1) + (1-\alpha) f(W_2)
        \end{align*}
    \end{proof}
    
    Thus, $f$ is smooth with gradient at $W\in \R^{|S|}$ given by $\nabla f(W) = - \zeta_{T_{W}}$.
    Recall that because of Lemma \ref{lemma::1step}, to solve our problem (\ref{eq::main}) we need to find a weight vector $W^*$ satisfying ${T_{W^*}}_{\#}(\mu) = \nu$. 
    In other words, it should hold $\mu(T_{W^*}^{-1} (s)) = \nu_s $ for all $s\in S$. \\
    Consider now the function
    %
    \[H: \R^{|S|} \to \R, \quad W \mapsto f(W) + \langle \nu, W \rangle = \langle \nu - \zeta_{T_{W}}, W \rangle + Q(T_W). \]
    %
    This function is concave and differentiable as a sum of concave differentiable functions. Furthermore we have $\nabla H(W) = \nu - \zeta_{T_{W}}$ and hence also
    \[ {T_{W}}_{\#}\mu(s) =\mu(T_W^{-1}(s)) = \nu_s \quad \forall s\in S \quad  \Leftrightarrow \quad  \zeta_{T_{W}} = \nu \quad  \Leftrightarrow \quad \nabla H(W) = 0. \]
    %
    Thus we see that finding a solution of our original problem is in deed equivalent to finding a maximum of the concave function $H$. We can compute
    \begin{align*}
        \frac{\partial H}{W(s)}  &=  \frac{\partial f(W)}{W(s)}  + \frac{\partial \langle \nu, W \rangle}{W(s)} \\
                                &= -\mu(T_W^{-1}(s)) + W(s) \\
                                &= -\mu(\reg(s)) + W(s).
    \end{align*}
    
    This optimization problem has a probabilistic interpretation. If we realize $X$ as a random variable of distribution $\mu$, i.e $X \sim \mu$. Then, we define 
    %
    \[h_W^\nu(x) \coloneqq \min_{s\in S} \|x-s\|^2 - W(s) + \langle W, \nu  \rangle = \|x-T_W(x)\|^2 -W(s) + \langle W, \nu  \rangle, \]
    %
    we have
    %\[h_W(x, s) = \min_{s\in S} \|x-s\|^2 - W(s) + \langle W, \nu  \rangle \]
    \begin{align*}
        \E[h_W^\nu(X)] &= \integral{\R^d} {\min_{s\in S} \|X-s\|^2 - W(s)} {\mu} + \integral{\R^d} {\langle W, \nu  \rangle } {\mu} \\
                    &= \integral{\R^d} {\|x-T_W(X)\|^2 -W(s)} {\mu} + \langle W, \nu  \rangle  = H(W).
    \end{align*}

    In other words, our problem can be stated as minimizing the expected value of $h_W^\nu(X)$.
     
    
\end{document}

